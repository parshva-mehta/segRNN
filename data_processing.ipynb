{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2202b95-bff9-4b44-8a29-d0b27d8108c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444d6b91-c49d-4a6f-bb7b-ceeaa97398bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"JRB.csv\", low_memory=False)  # force to str\n",
    "# df[df['station'] == 'JRB'].shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800f972e-509e-40f2-bb51-5d590cffee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'your_dataset.csv' with your actual data file\n",
    "df = pd.read_csv('JRB.csv', low_memory=False)\n",
    "\n",
    "# Step 1: Ensure 'valid' column is datetime and sort the DataFrame\n",
    "df['valid'] = pd.to_datetime(df['valid'])\n",
    "df = df.sort_values(by=['station', 'valid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0dbc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>valid</th>\n",
       "      <th>tmpf</th>\n",
       "      <th>dwpf</th>\n",
       "      <th>relh</th>\n",
       "      <th>drct</th>\n",
       "      <th>sknt</th>\n",
       "      <th>p01i</th>\n",
       "      <th>alti</th>\n",
       "      <th>mslp</th>\n",
       "      <th>...</th>\n",
       "      <th>wxcodes</th>\n",
       "      <th>ice_accretion_1hr</th>\n",
       "      <th>ice_accretion_3hr</th>\n",
       "      <th>ice_accretion_6hr</th>\n",
       "      <th>peak_wind_gust</th>\n",
       "      <th>peak_wind_drct</th>\n",
       "      <th>peak_wind_time</th>\n",
       "      <th>feel</th>\n",
       "      <th>metar</th>\n",
       "      <th>snowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 08:15:00</td>\n",
       "      <td>73.40</td>\n",
       "      <td>66.20</td>\n",
       "      <td>78.19</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>73.40</td>\n",
       "      <td>KJRB 211215Z 24005KT 10SM CLR 23/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 08:35:00</td>\n",
       "      <td>73.40</td>\n",
       "      <td>66.20</td>\n",
       "      <td>78.19</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>73.40</td>\n",
       "      <td>KJRB 211235Z 24005KT 10SM CLR 23/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 08:55:00</td>\n",
       "      <td>75.20</td>\n",
       "      <td>66.20</td>\n",
       "      <td>73.61</td>\n",
       "      <td>240.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>75.20</td>\n",
       "      <td>KJRB 211255Z 24006KT 10SM CLR 24/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 09:15:00</td>\n",
       "      <td>75.20</td>\n",
       "      <td>66.20</td>\n",
       "      <td>73.61</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>75.20</td>\n",
       "      <td>KJRB 211315Z 24005KT 10SM CLR 24/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 09:35:00</td>\n",
       "      <td>78.80</td>\n",
       "      <td>66.20</td>\n",
       "      <td>65.33</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.16</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>80.82</td>\n",
       "      <td>KJRB 211335Z 24005KT 10SM CLR 26/19 A3016 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station               valid   tmpf   dwpf   relh    drct  sknt  p01i   alti  \\\n",
       "0     JRB 2016-07-21 08:15:00  73.40  66.20  78.19  240.00  5.00  0.00  30.17   \n",
       "1     JRB 2016-07-21 08:35:00  73.40  66.20  78.19  240.00  5.00  0.00  30.17   \n",
       "2     JRB 2016-07-21 08:55:00  75.20  66.20  73.61  240.00  6.00  0.00  30.17   \n",
       "3     JRB 2016-07-21 09:15:00  75.20  66.20  73.61  240.00  5.00  0.00  30.17   \n",
       "4     JRB 2016-07-21 09:35:00  78.80  66.20  65.33  240.00  5.00  0.00  30.16   \n",
       "\n",
       "  mslp  ... wxcodes ice_accretion_1hr ice_accretion_3hr ice_accretion_6hr  \\\n",
       "0    M  ...       M                 M                 M                 M   \n",
       "1    M  ...       M                 M                 M                 M   \n",
       "2    M  ...       M                 M                 M                 M   \n",
       "3    M  ...       M                 M                 M                 M   \n",
       "4    M  ...       M                 M                 M                 M   \n",
       "\n",
       "  peak_wind_gust peak_wind_drct peak_wind_time   feel  \\\n",
       "0              M              M              M  73.40   \n",
       "1              M              M              M  73.40   \n",
       "2              M              M              M  75.20   \n",
       "3              M              M              M  75.20   \n",
       "4              M              M              M  80.82   \n",
       "\n",
       "                                               metar snowdepth  \n",
       "0  KJRB 211215Z 24005KT 10SM CLR 23/19 A3017 RMK AO1         M  \n",
       "1  KJRB 211235Z 24005KT 10SM CLR 23/19 A3017 RMK AO1         M  \n",
       "2  KJRB 211255Z 24006KT 10SM CLR 24/19 A3017 RMK AO1         M  \n",
       "3  KJRB 211315Z 24005KT 10SM CLR 24/19 A3017 RMK AO1         M  \n",
       "4  KJRB 211335Z 24005KT 10SM CLR 26/19 A3016 RMK AO1         M  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c993893a-b1e6-41b5-8243-6799fb89d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neela\\AppData\\Local\\Temp\\ipykernel_38432\\2860399940.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[continuous_cols] = df[continuous_cols].replace(placeholders, np.nan).astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Replace placeholders with np.nan in continuous columns\n",
    "continuous_cols = ['tmpf', 'dwpf', 'relh', 'feel', 'drct', 'sknt', 'gust',\n",
    "                   'peak_wind_gust', 'peak_wind_drct', 'alti', 'mslp', 'vsby',\n",
    "                   'p01i', 'ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr',\n",
    "                   'skyl1', 'skyl2', 'skyl3', 'skyl4', 'snowdepth', 'peak_wind_time']\n",
    "\n",
    "# List of placeholders to replace\n",
    "placeholders = ['M', 'T', '', 'NaN', 'NULL', 'None']\n",
    "\n",
    "# Replace placeholders with np.nan\n",
    "df[continuous_cols] = df[continuous_cols].replace(placeholders, np.nan).astype(str)\n",
    "\n",
    "# Convert continuous columns to numeric, coercing errors to np.nan\n",
    "for col in continuous_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d200400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in continuous columns before processing:\n",
      "tmpf                   6865\n",
      "dwpf                   7015\n",
      "relh                   7112\n",
      "feel                   7125\n",
      "drct                  25052\n",
      "sknt                   5024\n",
      "gust                 101714\n",
      "peak_wind_gust       108345\n",
      "peak_wind_drct       108345\n",
      "alti                  12732\n",
      "mslp                  37235\n",
      "vsby                   9941\n",
      "p01i                   7730\n",
      "ice_accretion_1hr    114664\n",
      "ice_accretion_3hr    114664\n",
      "ice_accretion_6hr    114664\n",
      "skyl1                 54223\n",
      "skyl2                 93650\n",
      "skyl3                107296\n",
      "skyl4                114664\n",
      "snowdepth            114664\n",
      "peak_wind_time       114664\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in continuous columns before processing:\")\n",
    "print(df[continuous_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd125b17-a291-4804-9adc-259aa99af2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in continuous columns after processing:\n",
      "skyl1             0\n",
      "tmpf              0\n",
      "mslp              0\n",
      "peak_wind_gust    0\n",
      "sknt              0\n",
      "alti              0\n",
      "dwpf              0\n",
      "skyl2             0\n",
      "peak_wind_drct    0\n",
      "p01i              0\n",
      "relh              0\n",
      "drct              0\n",
      "gust              0\n",
      "feel              0\n",
      "vsby              0\n",
      "skyl3             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Handle missing values in continuous variables\n",
    "# Apply linear interpolation within each station group using transform\n",
    "df[continuous_cols] = df.groupby('station')[continuous_cols].transform(\n",
    "    lambda group: group.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Handle any remaining missing values with forward and backward fill using transform\n",
    "df[continuous_cols] = df.groupby('station')[continuous_cols].transform(\n",
    "    lambda group: group.ffill().bfill()\n",
    ")\n",
    "\n",
    "# all of these columns have all nans\n",
    "bad_columns = bad_columns = [col for col in df.columns if df[col].isnull().sum() == df.shape[0]]\n",
    "# for JRB, this is ['ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr', 'skyl4', 'snowdepth', 'peak_wind_time']\n",
    "df.drop(columns=bad_columns, inplace=True)\n",
    "continuous_cols = list(set(continuous_cols) - set(bad_columns))\n",
    "\n",
    "\n",
    "# Verify missing values are filled\n",
    "print(\"Missing values in continuous columns after processing:\")\n",
    "print(df[continuous_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ba1dd0-f082-475f-ab51-527f8d9e5574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  station skyc1 skyc2 skyc3 skyc4 wxcodes\n",
      "0     JRB   CLR     M     M     M       M\n",
      "1     JRB   CLR     M     M     M       M\n",
      "2     JRB   CLR     M     M     M       M\n",
      "3     JRB   CLR     M     M     M       M\n",
      "4     JRB   CLR     M     M     M       M\n",
      "   station  skyc1  skyc2  skyc3  skyc4  wxcodes\n",
      "0        0      1      2      1      0       26\n",
      "1        0      1      2      1      0       26\n",
      "2        0      1      2      1      0       26\n",
      "3        0      1      2      1      0       26\n",
      "4        0      1      2      1      0       26\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Encode categorical variables\n",
    "# Categorical columns\n",
    "categorical_cols = ['station', 'skyc1', 'skyc2', 'skyc3', 'skyc4', 'wxcodes']\n",
    "\n",
    "# Fill missing values in categorical columns with a placeholder\n",
    "df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "print(df[categorical_cols].head())\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le  # Save the encoder if needed later\n",
    "\n",
    "print(df[categorical_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6609200a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metar', 'valid'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.columns) - set(df[continuous_cols].columns) - set(df[categorical_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d77192-039b-4a7e-8e4b-f543ce63b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114664, 22)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Feature scaling\n",
    "# List of all features (excluding 'valid' and 'metar')\n",
    "feature_cols = continuous_cols + categorical_cols\n",
    "print(df[feature_cols].shape)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bb536-b26d-4e1f-9521-baf8eaa878f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114640, 24, 22)\n",
      "(114640, 1)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Prepare sequences for LSTM input\n",
    "# Assuming we are predicting 'tmpf' (temperature) as the target variable\n",
    "# and using previous 24 time steps/8 hours (n_steps_in) to predict the next time step/20 minutes from now (n_steps_out)\n",
    "# create sliding window sequences X: (114640, 24, features), y: (114640, 1)\n",
    "\n",
    "n_steps_in = 24  # Number of past time steps\n",
    "n_steps_out = 1  # Number of future time steps to predict\n",
    "\n",
    "# We'll create sequences for each station separately\n",
    "def create_sequences(data, n_steps_in, n_steps_out, target_col):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:(i + n_steps_in), :])\n",
    "        y.append(data[(i + n_steps_in):(i + n_steps_in + n_steps_out), target_col])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare data for each station\n",
    "X_list = []\n",
    "y_list = []\n",
    "stations = df['station'].unique()\n",
    "\n",
    "for station in stations:\n",
    "    station_data = df[df['station'] == station]\n",
    "    station_data = station_data.reset_index(drop=True)\n",
    "    data_values = station_data[feature_cols].values\n",
    "    target_col_index = feature_cols.index('tmpf')  # Index of target variable in features\n",
    "\n",
    "    X_station, y_station = create_sequences(data_values, n_steps_in, n_steps_out, target_col_index)\n",
    "    X_list.append(X_station)\n",
    "    y_list.append(y_station)\n",
    "\n",
    "\n",
    "# Concatenate data from all stations\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c60c4d2-3530-4638-baa3-d182686746da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split the data into training and testing sets\n",
    "# Since it's time-series data, we'll use the first 80% for training and the rest for testing\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Now the data is ready for training the LSTM model\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b82caf3-0b75-4599-80b7-ed39d935ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n",
      "inputsize is 22\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = WeatherDataset(X_train, y_train)\n",
    "test_dataset = WeatherDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 8: Define and Train an LSTM Model Using PyTorch\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # batch_first=True: input and output tensors are of shape (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]  # Take the output from the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device is {device}')\n",
    "\n",
    "input_size = X.shape[2]  # Number of features\n",
    "print(f'inputsize is {input_size}')\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "output_size = n_steps_out  # Number of outputs\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b710219-3a69-4677-9c2e-438bccf7115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0865\n",
      "Epoch [2/50], Loss: 0.0409\n",
      "Epoch [3/50], Loss: 0.0314\n",
      "Epoch [4/50], Loss: 0.0441\n",
      "Epoch [5/50], Loss: 0.0321\n",
      "Epoch [6/50], Loss: 0.0180\n",
      "Epoch [7/50], Loss: 0.0144\n",
      "Epoch [8/50], Loss: 0.0184\n",
      "Epoch [9/50], Loss: 0.0163\n",
      "Epoch [10/50], Loss: 0.0141\n",
      "Epoch [11/50], Loss: 0.0135\n",
      "Epoch [12/50], Loss: 0.0133\n",
      "Epoch [13/50], Loss: 0.0125\n",
      "Epoch [14/50], Loss: 0.0144\n",
      "Epoch [15/50], Loss: 0.0125\n",
      "Epoch [16/50], Loss: 0.0121\n",
      "Epoch [17/50], Loss: 0.0155\n",
      "Epoch [18/50], Loss: 0.0143\n",
      "Epoch [19/50], Loss: 0.0216\n",
      "Epoch [20/50], Loss: 0.0146\n",
      "Epoch [21/50], Loss: 0.0122\n",
      "Epoch [22/50], Loss: 0.0117\n",
      "Epoch [23/50], Loss: 0.0117\n",
      "Epoch [24/50], Loss: 0.0122\n",
      "Epoch [25/50], Loss: 0.0130\n",
      "Epoch [26/50], Loss: 0.0124\n",
      "Epoch [27/50], Loss: 0.0120\n",
      "Epoch [28/50], Loss: 0.0117\n",
      "Epoch [29/50], Loss: 0.0113\n",
      "Epoch [30/50], Loss: 0.0116\n",
      "Epoch [31/50], Loss: 0.0108\n",
      "Epoch [32/50], Loss: 0.0107\n",
      "Epoch [33/50], Loss: 0.0109\n",
      "Epoch [34/50], Loss: 0.0130\n",
      "Epoch [35/50], Loss: 0.0104\n",
      "Epoch [36/50], Loss: 0.0107\n",
      "Epoch [37/50], Loss: 0.0108\n",
      "Epoch [38/50], Loss: 0.0100\n",
      "Epoch [39/50], Loss: 0.0100\n",
      "Epoch [40/50], Loss: 0.0096\n",
      "Epoch [41/50], Loss: 0.0099\n",
      "Epoch [42/50], Loss: 0.0091\n",
      "Epoch [43/50], Loss: 0.0096\n",
      "Epoch [44/50], Loss: 0.0124\n",
      "Epoch [45/50], Loss: 0.0091\n",
      "Epoch [46/50], Loss: 0.0093\n",
      "Epoch [47/50], Loss: 0.0093\n",
      "Epoch [48/50], Loss: 0.0095\n",
      "Epoch [49/50], Loss: 0.0090\n",
      "Epoch [50/50], Loss: 0.0089\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Training the model\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)  # inputs shape: [batch_size, seq_length, input_size]\n",
    "        targets = targets.to(device)  # targets shape: [batch_size, output_size]\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f998f0c-2793-42e8-b393-969b82a7522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.0054\n",
      "Test MAE Loss: 0.0542\n",
      "Model training and evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        actuals.append(targets.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    \n",
    "    mse_loss = criterion(torch.tensor(predictions), torch.tensor(actuals))\n",
    "    mae = nn.L1Loss()\n",
    "    mae_loss = mae(torch.tensor(predictions), torch.tensor(actuals))  # uses __call__\n",
    "    print(f'Test MSE Loss: {mse_loss.item():.4f}')\n",
    "    print(f'Test MAE Loss: {mae_loss.item():.4f}')\n",
    "\n",
    "# Optional: Save the model\n",
    "torch.save(model.state_dict(), 'lstm_weather_model.pth')\n",
    "\n",
    "print(\"Model training and evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_projects",
   "language": "python",
   "name": "torch_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
