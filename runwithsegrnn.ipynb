{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d16a89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from segrnn import SegRNNModel\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "800f972e-509e-40f2-bb51-5d590cffee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Replace 'your_dataset.csv' with your actual data file\n",
    "df = pd.read_csv('JRB.csv', low_memory=False)\n",
    "\n",
    "# Step 1: Ensure 'valid' column is datetime and sort the DataFrame\n",
    "df['valid'] = pd.to_datetime(df['valid'])\n",
    "df = df.sort_values(by=['station', 'valid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e0dbc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>valid</th>\n",
       "      <th>tmpf</th>\n",
       "      <th>dwpf</th>\n",
       "      <th>relh</th>\n",
       "      <th>drct</th>\n",
       "      <th>sknt</th>\n",
       "      <th>p01i</th>\n",
       "      <th>alti</th>\n",
       "      <th>mslp</th>\n",
       "      <th>...</th>\n",
       "      <th>wxcodes</th>\n",
       "      <th>ice_accretion_1hr</th>\n",
       "      <th>ice_accretion_3hr</th>\n",
       "      <th>ice_accretion_6hr</th>\n",
       "      <th>peak_wind_gust</th>\n",
       "      <th>peak_wind_drct</th>\n",
       "      <th>peak_wind_time</th>\n",
       "      <th>feel</th>\n",
       "      <th>metar</th>\n",
       "      <th>snowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 08:15:00</td>\n",
       "      <td>73.40</td>\n",
       "      <td>66.20</td>\n",
       "      <td>78.19</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>73.40</td>\n",
       "      <td>KJRB 211215Z 24005KT 10SM CLR 23/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 08:35:00</td>\n",
       "      <td>73.40</td>\n",
       "      <td>66.20</td>\n",
       "      <td>78.19</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>73.40</td>\n",
       "      <td>KJRB 211235Z 24005KT 10SM CLR 23/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 08:55:00</td>\n",
       "      <td>75.20</td>\n",
       "      <td>66.20</td>\n",
       "      <td>73.61</td>\n",
       "      <td>240.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>75.20</td>\n",
       "      <td>KJRB 211255Z 24006KT 10SM CLR 24/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 09:15:00</td>\n",
       "      <td>75.20</td>\n",
       "      <td>66.20</td>\n",
       "      <td>73.61</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.17</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>75.20</td>\n",
       "      <td>KJRB 211315Z 24005KT 10SM CLR 24/19 A3017 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JRB</td>\n",
       "      <td>2016-07-21 09:35:00</td>\n",
       "      <td>78.80</td>\n",
       "      <td>66.20</td>\n",
       "      <td>65.33</td>\n",
       "      <td>240.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.16</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>80.82</td>\n",
       "      <td>KJRB 211335Z 24005KT 10SM CLR 26/19 A3016 RMK AO1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station               valid   tmpf   dwpf   relh    drct  sknt  p01i   alti  \\\n",
       "0     JRB 2016-07-21 08:15:00  73.40  66.20  78.19  240.00  5.00  0.00  30.17   \n",
       "1     JRB 2016-07-21 08:35:00  73.40  66.20  78.19  240.00  5.00  0.00  30.17   \n",
       "2     JRB 2016-07-21 08:55:00  75.20  66.20  73.61  240.00  6.00  0.00  30.17   \n",
       "3     JRB 2016-07-21 09:15:00  75.20  66.20  73.61  240.00  5.00  0.00  30.17   \n",
       "4     JRB 2016-07-21 09:35:00  78.80  66.20  65.33  240.00  5.00  0.00  30.16   \n",
       "\n",
       "  mslp  ... wxcodes ice_accretion_1hr ice_accretion_3hr ice_accretion_6hr  \\\n",
       "0    M  ...       M                 M                 M                 M   \n",
       "1    M  ...       M                 M                 M                 M   \n",
       "2    M  ...       M                 M                 M                 M   \n",
       "3    M  ...       M                 M                 M                 M   \n",
       "4    M  ...       M                 M                 M                 M   \n",
       "\n",
       "  peak_wind_gust peak_wind_drct peak_wind_time   feel  \\\n",
       "0              M              M              M  73.40   \n",
       "1              M              M              M  73.40   \n",
       "2              M              M              M  75.20   \n",
       "3              M              M              M  75.20   \n",
       "4              M              M              M  80.82   \n",
       "\n",
       "                                               metar snowdepth  \n",
       "0  KJRB 211215Z 24005KT 10SM CLR 23/19 A3017 RMK AO1         M  \n",
       "1  KJRB 211235Z 24005KT 10SM CLR 23/19 A3017 RMK AO1         M  \n",
       "2  KJRB 211255Z 24006KT 10SM CLR 24/19 A3017 RMK AO1         M  \n",
       "3  KJRB 211315Z 24005KT 10SM CLR 24/19 A3017 RMK AO1         M  \n",
       "4  KJRB 211335Z 24005KT 10SM CLR 26/19 A3016 RMK AO1         M  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c993893a-b1e6-41b5-8243-6799fb89d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neela\\AppData\\Local\\Temp\\ipykernel_29008\\2860399940.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[continuous_cols] = df[continuous_cols].replace(placeholders, np.nan).astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Replace placeholders with np.nan in continuous columns\n",
    "continuous_cols = ['tmpf', 'dwpf', 'relh', 'feel', 'drct', 'sknt', 'gust',\n",
    "                   'peak_wind_gust', 'peak_wind_drct', 'alti', 'mslp', 'vsby',\n",
    "                   'p01i', 'ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr',\n",
    "                   'skyl1', 'skyl2', 'skyl3', 'skyl4', 'snowdepth', 'peak_wind_time']\n",
    "\n",
    "# List of placeholders to replace\n",
    "placeholders = ['M', 'T', '', 'NaN', 'NULL', 'None']\n",
    "\n",
    "# Replace placeholders with np.nan\n",
    "df[continuous_cols] = df[continuous_cols].replace(placeholders, np.nan).astype(str)\n",
    "\n",
    "# Convert continuous columns to numeric, coercing errors to np.nan\n",
    "for col in continuous_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d200400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in continuous columns before processing:\n",
      "tmpf                   6865\n",
      "dwpf                   7015\n",
      "relh                   7112\n",
      "feel                   7125\n",
      "drct                  25052\n",
      "sknt                   5024\n",
      "gust                 101714\n",
      "peak_wind_gust       108345\n",
      "peak_wind_drct       108345\n",
      "alti                  12732\n",
      "mslp                  37235\n",
      "vsby                   9941\n",
      "p01i                   7730\n",
      "ice_accretion_1hr    114664\n",
      "ice_accretion_3hr    114664\n",
      "ice_accretion_6hr    114664\n",
      "skyl1                 54223\n",
      "skyl2                 93650\n",
      "skyl3                107296\n",
      "skyl4                114664\n",
      "snowdepth            114664\n",
      "peak_wind_time       114664\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in continuous columns before processing:\")\n",
    "print(df[continuous_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd125b17-a291-4804-9adc-259aa99af2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan thresh is 57332.0\n",
      "bad columns are ['gust', 'skyl2', 'skyl3', 'skyl4', 'ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr', 'peak_wind_gust', 'peak_wind_drct', 'peak_wind_time', 'snowdepth']\n",
      "Remaining continuous columns: ['feel', 'dwpf', 'p01i', 'relh', 'mslp', 'alti', 'vsby', 'tmpf', 'drct', 'sknt']\n",
      "Missing values in continuous columns after processing:\n",
      "feel    0\n",
      "dwpf    0\n",
      "p01i    0\n",
      "relh    0\n",
      "mslp    0\n",
      "alti    0\n",
      "vsby    0\n",
      "tmpf    0\n",
      "drct    0\n",
      "sknt    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Handle missing values in continuous variables\n",
    "\n",
    "\n",
    "# Identify columns to drop due to high NaN count\n",
    "nan_threshold = df.shape[0] / 2                     # Remove columns with more than 50% missing values\n",
    "print(f\"nan thresh is {nan_threshold}\")\n",
    "bad_columns = [col for col in df.columns if df[col].isnull().sum() >= nan_threshold]\n",
    "print(f\"bad columns are {bad_columns}\")\n",
    "\n",
    "# Add less relevant and irrelevant features to the removal list\n",
    "irrelevant_features = [\n",
    "    'ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr',  # Fully NaN\n",
    "    'skyl1', 'skyl2', 'skyl3', 'skyl4',  # Less relevant (Sky level altitudes)\n",
    "    'skyc1', 'skyc2', 'skyc3', 'skyc4',  # Less relevant (Sky coverage)\n",
    "    'wxcodes',  # Categorical, redundant with precipitation/visibility\n",
    "    'metar'  # Text format, unusable directly\n",
    "]\n",
    "\n",
    "# Combine both lists and ensure no duplicates\n",
    "columns_to_remove = list(set(bad_columns + irrelevant_features))\n",
    "df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Update continuous columns to exclude removed columns\n",
    "continuous_cols = list(set(continuous_cols) - set(columns_to_remove))\n",
    "print(f\"Remaining continuous columns: {continuous_cols}\")\n",
    "\n",
    "\n",
    "# Apply linear interpolation within each station group using transform\n",
    "df[continuous_cols] = df.groupby('station')[continuous_cols].transform(\n",
    "    lambda group: group.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Handle any remaining missing values with forward and backward fill using transform\n",
    "df[continuous_cols] = df.groupby('station')[continuous_cols].transform(\n",
    "    lambda group: group.ffill().bfill()\n",
    ")\n",
    "\n",
    "\n",
    "# Verify missing values are filled\n",
    "print(\"Missing values in continuous columns after processing:\")\n",
    "print(df[continuous_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d77192-039b-4a7e-8e4b-f543ce63b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114664, 10)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Feature scaling\n",
    "# List of all features (excluding 'valid' and 'metar')\n",
    "feature_cols = continuous_cols #+ categorical_cols\n",
    "print(df[feature_cols].shape)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "678bb536-b26d-4e1f-9521-baf8eaa878f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114640, 24, 10)\n",
      "(114640, 10)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Prepare sequences for LSTM input\n",
    "# Assuming we are predicting 'tmpf' (temperature) as the target variable\n",
    "# and using previous 24 time steps/8 hours (n_steps_in) to predict the next time step/20 minutes from now (n_steps_out)\n",
    "# create sliding window sequences X: (114640, 24, 10), y: (114640, 10)\n",
    "\n",
    "n_steps_in = 24  # Number of past time steps\n",
    "n_steps_out = 1  # Number of future time steps to predict\n",
    "\n",
    "# We'll create sequences for each station separately\n",
    "def create_sequences(data, n_steps_in, n_steps_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:(i + n_steps_in), :])\n",
    "        y.append(data[(i + n_steps_in):(i + n_steps_in + n_steps_out), :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare data for each station\n",
    "X_list = []\n",
    "y_list = []\n",
    "stations = df['station'].unique()\n",
    "\n",
    "for station in stations:\n",
    "    station_data = df[df['station'] == station]\n",
    "    station_data = station_data.reset_index(drop=True)\n",
    "    data_values = station_data[feature_cols].values\n",
    "    # target_col_index = feature_cols.index('tmpf')  # Index of target variable in features\n",
    "\n",
    "    X_station, y_station = create_sequences(data_values, n_steps_in, n_steps_out)\n",
    "    X_list.append(X_station)\n",
    "    y_list.append(y_station)\n",
    "\n",
    "\n",
    "# Concatenate data from all stations\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "\n",
    "if n_steps_out == 1:\n",
    "    y = y.squeeze(1)  # Shape becomes (num_samples, num_features) = (114640, 10) for JRB\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c60c4d2-3530-4638-baa3-d182686746da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split the data into training and testing sets\n",
    "# Since it's time-series data, we'll use the first 80% for training and the rest for testing\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Now the data is ready for training the LSTM model\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b82caf3-0b75-4599-80b7-ed39d935ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model     | SegRNN  | 2.4 M  | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.544     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d65b19323431483d75d1a452e7de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cd4ddd330f4930955d5397dfd48c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12028153985738754    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12028153985738754   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.12028153985738754}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = WeatherDataset(X_train, y_train)\n",
    "test_dataset = WeatherDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Hyperparameters for SegRNN\n",
    "input_size = X.shape[2]  # Number of features\n",
    "hidden_size = 512  # Based on the SEGRNN paper\n",
    "output_size = X.shape[2]  # Predict all features\n",
    "segment_length = 8  # Based on the SEGRNN paper\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize SegRNNModel\n",
    "model = SegRNNModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    segment_length=segment_length,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\"logs\", name=\"segrnn_experiment\")\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"segrnn-{epoch:02d}-{val_loss:.4f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Trainer with logging and checkpointing\n",
    "trainer = Trainer(\n",
    "    max_epochs=4,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader)\n",
    "\n",
    "# Optional: Evaluate on the test set\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c726626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b8fed1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23700), started 0:06:17 ago. (Use '!kill 23700' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7df633dc072078e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7df633dc072078e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --port=6006\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_projects",
   "language": "python",
   "name": "torch_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
