{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f972e-509e-40f2-bb51-5d590cffee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'your_dataset.csv' with your actual data file\n",
    "df = pd.read_csv('JRB.csv', low_memory=False)\n",
    "\n",
    "# Step 1: Ensure 'valid' column is datetime and sort the DataFrame\n",
    "df['valid'] = pd.to_datetime(df['valid'])\n",
    "df = df.sort_values(by=['station', 'valid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993893a-b1e6-41b5-8243-6799fb89d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Replace placeholders with np.nan in continuous columns\n",
    "continuous_cols = ['tmpf', 'dwpf', 'relh', 'feel', 'drct', 'sknt', 'gust',\n",
    "                   'peak_wind_gust', 'peak_wind_drct', 'alti', 'mslp', 'vsby',\n",
    "                   'p01i', 'ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr',\n",
    "                   'skyl1', 'skyl2', 'skyl3', 'skyl4', 'snowdepth', 'peak_wind_time']\n",
    "\n",
    "# List of placeholders to replace\n",
    "placeholders = ['M', 'T', '', 'NaN', 'NULL', 'None']\n",
    "\n",
    "# Replace placeholders with np.nan\n",
    "df[continuous_cols] = df[continuous_cols].replace(placeholders, np.nan).astype(str)\n",
    "\n",
    "# Convert continuous columns to numeric, coercing errors to np.nan\n",
    "for col in continuous_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    \n",
    "print(\"Missing values in continuous columns before processing:\")\n",
    "print(df[continuous_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd125b17-a291-4804-9adc-259aa99af2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handle missing values in continuous variables\n",
    "# Apply linear interpolation within each station group using transform\n",
    "df[continuous_cols] = df.groupby('station')[continuous_cols].transform(\n",
    "    lambda group: group.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Handle any remaining missing values with forward and backward fill using transform\n",
    "df[continuous_cols] = df.groupby('station')[continuous_cols].transform(\n",
    "    lambda group: group.ffill().bfill()\n",
    ")\n",
    "\n",
    "# all of these columns have all nans\n",
    "bad_columns = bad_columns = [col for col in df.columns if df[col].isnull().sum() == df.shape[0]]\n",
    "# for JRB, this is ['ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr', 'skyl4', 'snowdepth', 'peak_wind_time']\n",
    "df.drop(columns=bad_columns, inplace=True)\n",
    "continuous_cols = list(set(continuous_cols) - set(bad_columns))\n",
    "\n",
    "\n",
    "# Verify missing values are filled\n",
    "print(\"Missing values in continuous columns after processing:\")\n",
    "print(df[continuous_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba1dd0-f082-475f-ab51-527f8d9e5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Encode categorical variables\n",
    "# Categorical columns\n",
    "categorical_cols = ['station', 'skyc1', 'skyc2', 'skyc3', 'skyc4', 'wxcodes']\n",
    "\n",
    "# Fill missing values in categorical columns with a placeholder\n",
    "df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n",
    "\n",
    "print(df[categorical_cols].head())\n",
    "\n",
    "# Apply label encoding to categorical columns\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le  # Save the encoder if needed later\n",
    "\n",
    "print(df[categorical_cols].head())\n",
    "set(df.columns) - set(df[continuous_cols].columns) - set(df[categorical_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d77192-039b-4a7e-8e4b-f543ce63b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Feature scaling\n",
    "# List of all features (excluding 'valid' and 'metar')\n",
    "feature_cols = continuous_cols + categorical_cols\n",
    "print(df[feature_cols].shape)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bb536-b26d-4e1f-9521-baf8eaa878f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare sequences for LSTM input\n",
    "# Assuming we are predicting 'tmpf' (temperature) as the target variable\n",
    "# and using previous 24 time steps/8 hours (n_steps_in) to predict the next time step/20 minutes from now (n_steps_out)\n",
    "# create sliding window sequences X: (114640, 24, features), y: (114640, 1)\n",
    "\n",
    "n_steps_in = 24  # Number of past time steps\n",
    "n_steps_out = 1  # Number of future time steps to predict\n",
    "\n",
    "# We'll create sequences for each station separately\n",
    "def create_sequences(data, n_steps_in, n_steps_out, target_col):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:(i + n_steps_in), :])\n",
    "        y.append(data[(i + n_steps_in):(i + n_steps_in + n_steps_out), target_col])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare data for each station\n",
    "X_list = []\n",
    "y_list = []\n",
    "stations = df['station'].unique()\n",
    "\n",
    "for station in stations:\n",
    "    station_data = df[df['station'] == station]\n",
    "    station_data = station_data.reset_index(drop=True)\n",
    "    data_values = station_data[feature_cols].values\n",
    "    target_col_index = feature_cols.index('tmpf')  # Index of target variable in features\n",
    "\n",
    "    X_station, y_station = create_sequences(data_values, n_steps_in, n_steps_out, target_col_index)\n",
    "    X_list.append(X_station)\n",
    "    y_list.append(y_station)\n",
    "\n",
    "\n",
    "# Concatenate data from all stations\n",
    "X = np.concatenate(X_list, axis=0)\n",
    "y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60c4d2-3530-4638-baa3-d182686746da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split the data into training and testing sets\n",
    "# Since it's time-series data, we'll use the first 80% for training and the rest for testing\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Now the data is ready for training the LSTM model\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82caf3-0b75-4599-80b7-ed39d935ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = WeatherDataset(X_train, y_train)\n",
    "test_dataset = WeatherDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 8: Define and Train an LSTM Model Using PyTorch\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # batch_first=True: input and output tensors are of shape (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]  # Take the output from the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device is {device}')\n",
    "\n",
    "input_size = X.shape[2]  # Number of features\n",
    "print(f'inputsize is {input_size}')\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "output_size = n_steps_out  # Number of outputs\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b710219-3a69-4677-9c2e-438bccf7115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Step 9: Training the model with logging\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Optional: Validation loss (calculated on test_loader)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f998f0c-2793-42e8-b393-969b82a7522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        actuals.append(targets.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    rmse = mean_squared_error(actuals, predictions, squared=False)\n",
    "\n",
    "    print(f\"Test R²: {r2:.4f}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Scatter plot of predicted vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actuals, predictions, alpha=0.6)\n",
    "plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', label='Ideal Fit')  # Ideal line\n",
    "plt.xlabel(\"Actual Temperatures\")\n",
    "plt.ylabel(\"Predicted Temperatures\")\n",
    "plt.title(\"Predicted vs. Actual Values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Time-series plot of predictions vs actuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals[:100], label=\"Actual\")\n",
    "plt.plot(predictions[:100], label=\"Predicted\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.title(\"Temperature Prediction Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
